{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations of Q-iteration and Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This function performs Q_iteration, based on Bellman optimality equation. It's an iterative update of the Q function. The input\n",
    "# parameters are the discount factor gamma, the number of iterations epochs and the reward matrix R. P corresponds to the \n",
    "# probability of having a broken robot, thus staying to the same state with probability P. The function returns the \n",
    "# converged Q-function. Convergence is assured by comparison to the previous iteration Q-function.\n",
    "def Q_iteration(gamma, epochs, R, P=None):\n",
    "    if P == None:\n",
    "        P = 0\n",
    "    Q = np.zeros_like(R)\n",
    "    Q_next = np.zeros_like(Q)\n",
    "    \n",
    "    # iterate for all states and all actions\n",
    "    for ii in range(epochs):\n",
    "        for s in range(Q.shape[1]):\n",
    "            #  if it's a terminal state continue\n",
    "            if s==0 or s==5:\n",
    "                    continue\n",
    "            for a in range(Q.shape[0]):\n",
    "                # perform the update\n",
    "                Q_next[a,s] = R[a,s] + gamma * np.amax([(1-P) * Q[:, 2*a - 1 + s] +  P * Q[:,s]])\n",
    "        \n",
    "        # compare to the value of the previous iteration and break if converged\n",
    "        if np.linalg.norm(Q-Q_next) < 1e-8:\n",
    "            break\n",
    "            \n",
    "        #update the value for the next iteration \n",
    "        #print(Q)\n",
    "        Q = np.array(Q_next, copy = True)\n",
    "        \n",
    "    Q = np.array(Q_next, copy = True)\n",
    "    return(Q)           \n",
    "    \n",
    "\n",
    "# The implementation of Q_learning algorithm. The input parameters are the discount factor gamma, the learning rate alpha, the \n",
    "# parameter epsilon of greedy research, the number of iterations epochs and the reward matrix R. P corresponds to the \n",
    "# probability of having a broken robot, thus staying to the same state with probability P.\n",
    "def Q_learning(gamma, alpha, epsilon, epochs, R, P = None):\n",
    "    if P == None:\n",
    "        P = 0\n",
    "    Q = np.zeros_like(R)\n",
    "    Q_next = np.zeros_like(Q)\n",
    "    \n",
    "    n_states = Q.shape[1] - 1\n",
    "    end_state = False\n",
    "    \n",
    "    # iterate for the set number of epochs\n",
    "    for ii in range(epochs):\n",
    "        # set an arbitrary initial state. Continue if the initial state is also a terminal state.\n",
    "        s = np.rint(n_states * np.random.rand()).astype(int)\n",
    "        if s==0 or s==5:\n",
    "                continue\n",
    "        # each episode include a number of steps updating the Q-function until the agent reaches a terminal state, when the\n",
    "        # running episode ends and we move to the next iteration\n",
    "        while(not end_state):\n",
    "            # set current Q function to the value extracted from the previous iteration.\n",
    "            Q = np.array(Q_next, copy = True)\n",
    "\n",
    "            # Apply ε-greedy research\n",
    "            t = np.random.rand()\n",
    "            if t<epsilon:\n",
    "                a = np.rint(np.random.rand()).astype(int)\n",
    "            else:\n",
    "                a = np.argmax([Q[:,s]])\n",
    "        \n",
    "            # Determine the next state based on action a. Then update the Q function.\n",
    "            # If we get a reward for ending up in a terminal state set end_state to true and continue with the next epoch\n",
    "            s_next = 2*a - 1 + s\n",
    "            Q_next[a,s] = Q[a,s] + alpha * (R[a,s] + gamma * np.amax([(1-P)*Q[:,s_next] + P*Q[:, s]]) - Q[a,s])\n",
    "            #Q_next[a,s] = Q[a,s] + alpha/(ii+1) * (R[a,s] + gamma * max(Q[:,s_next]) - Q[a,s])\n",
    "            if R[a,s]>0:\n",
    "                end_state = True   \n",
    "            s = s_next    \n",
    "        end_state = False\n",
    "                                       \n",
    "    Q = np.array(Q_next, copy = True)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 5. 0.]]\n",
      "[[0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 5. 5. 0.]]\n",
      "[[0. 1. 1. 1. 5. 0.]\n",
      " [0. 1. 5. 5. 5. 0.]]\n",
      "[[0. 1. 1. 5. 5. 0.]\n",
      " [0. 5. 5. 5. 5. 0.]]\n",
      "[[0. 1. 5. 5. 5. 0.]\n",
      " [0. 5. 5. 5. 5. 0.]]\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "gamma = 1\n",
    "\n",
    "Q = np.zeros((2,6))\n",
    "R = np.zeros_like(Q)\n",
    "R[0,1] = 1\n",
    "R[1,4] = 5\n",
    "\n",
    "QQ = Q_iteration(gamma, 1000, R, 0)\n",
    "print(np.round(QQ, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    1.176 0.484 0.712 1.73  0.   ]\n",
      " [0.    0.411 0.997 2.422 5.882 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "QQ = Q_learning(gamma, alpha, epsilon, 10000, R, 0.3)\n",
    "print(np.round(QQ, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def divQlearn(Q_star, gamma, alpha, epsilon, epochs, rep, R, P):\n",
    "    diff = np.zeros(rep)\n",
    "    mean_diff = np.zeros(len(epochs))\n",
    "    std_diff = np.zeros_like(mean_diff)\n",
    "    \n",
    "    for i in range(len(epochs)):\n",
    "        for j in range(rep):\n",
    "            Q = Q_learning(gamma, alpha, epsilon, epochs[i], R, 0)\n",
    "            diff[j] = np.linalg.norm(Q - Q_star)\n",
    "        mean_diff[i] = np.mean(diff)\n",
    "        std_diff[i] = np.std(diff)\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    plt.title('L2-norm difference berween Q-learning and groundtruth')\n",
    "    plt.xlabel(\"Number of interactions - epochs\")\n",
    "    plt.ylabel(\"L2-norm difference\")\n",
    "    aa = np.arange(len(epochs))\n",
    "    plt.fill_between(aa, mean_diff - std_diff,\n",
    "                     mean_diff + std_diff, alpha=0.1, color=\"r\")\n",
    "    plt.plot(aa, mean_diff, 'o-', color=\"r\", label = 'L2 diff')\n",
    "    \n",
    "    #plt.axis([0, 1, 0, 1], aspect = 'equal')\n",
    "    #axes.set_xticks(aa) #set the ticks to be a\n",
    "    #plt.axes.set_xticklabels(labels=['0', '0.1', '1', '10', '100', '1000']) # change the ticks' names to x\n",
    "    #labels = ['0', '0.1', '1', '10', '100', '1000']\n",
    "    #plt.xticks(aa, labels, rotation = 'horizontal')\n",
    "    plt.grid()\n",
    "    plt.legend(loc=\"best\")\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    1.    0.5   0.625 1.25  0.   ]\n",
      " [0.    6.25  1.25  2.5   5.    0.   ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a6cdbbcba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(1000, 10000, 1000)\n",
    "Q_star = np.array([[0.0, 1.0, 0.5, 0.625, 1.25, 0.0], [0.0, 6.25, 1.25, 2.5, 5.0, 0.0]])\n",
    "print(Q_star)\n",
    "\n",
    "gamma = 0.5\n",
    "alpha = 1\n",
    "epsilon = 0.3\n",
    "rep = 5\n",
    "\n",
    "divQlearn(Q_star, 0.5, alpha, epsilon, epochs, rep, R, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-87952cb9a398>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mQ_next\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 1000\n",
    "gamma = 0.5\n",
    "\n",
    "Q = np.zeros((2,6))\n",
    "Q_next = np.zeros_like(Q)\n",
    "R = np.zeros_like(Q)\n",
    "R[0,1] = 1\n",
    "R[1,4] = 5\n",
    "c = 0\n",
    "\n",
    "for ii in range(epochs):\n",
    "    c+=1\n",
    "    for s in range(Q.shape[1]):\n",
    "        if s==0 or s==5:\n",
    "                continue\n",
    "        for a in range(Q.shape[0]):\n",
    "            Q_next[a,s] = R[a,s] + gamma * max(Q[:, 2*a - 1 + s])\n",
    "    if np.linalg.norm(Q-Q_next) < 1e-16:\n",
    "        break\n",
    "    Q = np.array(Q_next, copy = True)\n",
    "            \n",
    "Q = Q_next        \n",
    "#print(Q)\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.     1.     0.725  1.1875 2.375  0.    ]\n",
      " [0.     0.7375 1.25   2.5    5.     0.    ]]\n"
     ]
    }
   ],
   "source": [
    "QQ = Q_iteration(gamma, epochs, R, 0.3)\n",
    "print(QQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.824 0.393 0.499 1.211 0.   ]\n",
      " [0.    0.368 0.698 1.696 4.118 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "# Implementation of Q-learning\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the discount factor gamma, the learning rate alpha, the parameter for ε-greedy epsilon and the number of iterations\n",
    "# for the Q-learning algorithm epochs\n",
    "gamma = 0.5\n",
    "alpha = 1\n",
    "epsilon = 0.1\n",
    "epochs = 2000 \n",
    "P = 0.3\n",
    "\n",
    "Q = np.zeros((2,6))\n",
    "Q_next = np.zeros_like(Q)\n",
    "R = np.zeros_like(Q)\n",
    "R[0,1] = 1\n",
    "R[1,4] = 5\n",
    "n_states = Q.shape[1] - 1\n",
    "end_state = False\n",
    "\n",
    "\n",
    "for ii in range(epochs):\n",
    "    # set an arbitrary initial state. Continue if the initial state is also a terminal state.\n",
    "    s = np.rint(n_states * np.random.rand()).astype(int)\n",
    "    if s==0 or s==5:\n",
    "            continue\n",
    "    while(not end_state):\n",
    "        # set current Q function to the value extracted from the previous iteration.\n",
    "        Q = np.array(Q_next, copy = True)\n",
    "\n",
    "        # Apply ε-greedy research\n",
    "        t = np.random.rand()\n",
    "        if t<epsilon:\n",
    "            a = np.rint(np.random.rand()).astype(int)\n",
    "        else:\n",
    "            a = np.argmax([Q[:,s]])\n",
    "        \n",
    "        # Determine the next state based on action a. Then update the Q function.\n",
    "        # If we get a reward for ending up in a terminal state set end_state to true and continue with the next epoch\n",
    "        s_next = 2*a - 1 + s\n",
    "        Q_next[a,s] = Q[a,s] + alpha * (P * gamma * max(Q[:, s]) + (1-P) * (R[a,s] + gamma * max(Q[:,s_next])) - Q[a,s])\n",
    "        #Q_next[a,s] = Q[a,s] + alpha/(ii+1) * (R[a,s] + gamma * max(Q[:,s_next]) - Q[a,s])\n",
    "        if R[a,s]>0:\n",
    "            end_state = True   \n",
    "        s = s_next    \n",
    "    end_state = False\n",
    "                                       \n",
    "\n",
    "Q = Q_next    \n",
    "print(np.round(Q,3))                \n",
    "                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous state - non deterministic transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 5.578089670481885\n",
      "Divergence 0.00017088174982911666\n",
      "Loss 77.20980680252865\n",
      "Divergence 0.027472527472527486\n",
      "Loss 79.34752487483807\n",
      "Divergence 0.0006105006105006083\n",
      "Loss 79.53295820328552\n",
      "Divergence 0.00017403411068569324\n",
      "Loss 79.56690331735255\n",
      "Divergence 8.779168788299301e-06\n",
      "Loss 79.60089990869498\n",
      "Divergence 8.228827227543464e-05\n",
      "Loss 79.6488154922247\n",
      "Divergence 3.867394768188471e-06\n",
      "Loss 79.6594318989987\n",
      "Divergence 3.132832080200362e-05\n",
      "Loss 79.66711539232838\n",
      "Divergence 2.636296530633686e-05\n",
      "Loss 79.67423915477988\n",
      "Divergence 2.4683313093016527e-06\n",
      "Loss 79.68072030805389\n",
      "Divergence 1.8526752630797574e-05\n",
      "Loss 79.68519918792036\n",
      "Divergence 1.953353908661125e-06\n",
      "Loss 79.68920344527331\n",
      "Divergence 1.7290269036585645e-06\n",
      "Loss 79.6939054773825\n",
      "Divergence 1.5922147069689137e-06\n",
      "Loss 79.69885992225026\n",
      "Divergence 1.0366346692098141e-05\n",
      "Loss 79.704630697613\n",
      "Divergence 1.2062580668507818e-06\n",
      "Loss 79.70680307192997\n",
      "Divergence 7.536817352767761e-06\n",
      "Loss 79.71265091915642\n",
      "Divergence 5.737748186298089e-06\n",
      "Loss 79.71444852069297\n",
      "Divergence 7.964827322543e-07\n",
      "Loss 79.71599254893886\n",
      "Divergence 7.438041117491809e-07\n",
      "Loss 79.71731325271489\n",
      "Divergence 6.904112918147557e-07\n",
      "Loss 79.71846431373287\n",
      "Divergence 4.220007562253866e-06\n",
      "Loss 79.72073908688976\n",
      "Divergence 3.6745198504911647e-06\n",
      "Loss 79.72276093334692\n",
      "Divergence 5.098399102681714e-07\n",
      "Loss 79.72373511071466\n",
      "Divergence 3.025700298333895e-06\n",
      "Loss 79.72447501015047\n",
      "Divergence 4.580839447989758e-07\n",
      "Loss 79.72535019398897\n",
      "Divergence 2.69337634116679e-06\n",
      "Loss 79.72623024985486\n",
      "Divergence 2.52390644181694e-06\n",
      "Loss 79.72691467338063\n",
      "Divergence 2.406250090234048e-06\n",
      "Loss 79.72763936980387\n",
      "Divergence 3.679782392387778e-07\n",
      "Loss 79.72825412421047\n",
      "Divergence 2.162704591854043e-06\n",
      "Loss 79.72962916624026\n",
      "Divergence 1.9277405723843247e-06\n",
      "Loss 79.73024216961028\n",
      "Divergence 3.033943762817404e-07\n",
      "Loss 79.73066127610025\n",
      "Divergence 1.7662814053667243e-06\n",
      "Loss 79.7312118329968\n",
      "Divergence 2.7570083151369235e-07\n",
      "Loss 79.73174688720471\n",
      "Divergence 1.5968675845462812e-06\n",
      "Loss 79.73221499106079\n",
      "Divergence 2.5290000434997385e-07\n",
      "Loss 79.73282104767789\n",
      "Divergence 1.4475877686942994e-06\n",
      "Loss 79.73326370391506\n",
      "Divergence 2.2218459896347414e-07\n",
      "Loss 79.73371262497001\n",
      "Divergence 1.3210144334035653e-06\n",
      "Loss 79.7340058938307\n",
      "Divergence 1.2786069117400045e-06\n",
      "Loss 79.73439286443445\n",
      "Divergence 1.999227498494468e-07\n",
      "Loss 79.73470235648601\n",
      "Divergence 1.9127473517056588e-07\n",
      "Loss 79.7350107400187\n",
      "Divergence 1.8396244811340204e-07\n",
      "Loss 79.7357317930302\n",
      "Divergence 1.060352732698295e-06\n",
      "Loss 79.73598727164902\n",
      "Divergence 1.0316506286259242e-06\n",
      "Loss 79.7363125113737\n",
      "Divergence 9.933856410476553e-07\n",
      "Loss 79.73653157078864\n",
      "Divergence 1.5372672000982246e-07\n",
      "Loss 79.73677495669135\n",
      "Divergence 1.4764493047841713e-07\n",
      "Loss 79.73732639128971\n",
      "Divergence 8.779042389079651e-07\n",
      "Loss 79.7376357115346\n",
      "Divergence 8.422425752949506e-07\n",
      "Loss 79.73787621748048\n",
      "Divergence 1.327619260038074e-07\n",
      "Loss 79.73837045892837\n",
      "Divergence 7.632375928095549e-07\n",
      "Loss 79.73859415626198\n",
      "Divergence 1.2136261084955786e-07\n",
      "Loss 79.73883299010801\n",
      "Divergence 1.1870132207155793e-07\n",
      "Loss 79.73928075363798\n",
      "Divergence 6.676965217818596e-07\n",
      "Loss 79.73948036712555\n",
      "Divergence 6.48596592636632e-07\n",
      "Loss 79.73968880926162\n",
      "Divergence 6.28072688610903e-07\n",
      "Loss 79.73991163611493\n",
      "Divergence 6.080834232080438e-07\n",
      "Loss 79.74010934924415\n",
      "Divergence 5.902483883267994e-07\n",
      "Loss 79.74029587197704\n",
      "Divergence 9.998594197653966e-08\n",
      "Loss 79.740491374242\n",
      "Divergence 9.787022642863312e-08\n",
      "Loss 79.7406503363326\n",
      "Divergence 5.422798224619368e-07\n",
      "Loss 79.74082834177086\n",
      "Divergence 9.383539020509177e-08\n",
      "Loss 79.74097825351004\n",
      "Divergence 9.168839223120838e-08\n",
      "Loss 79.74112843020481\n",
      "Divergence 5.02150207187281e-07\n",
      "Loss 79.74122623337199\n",
      "Divergence 8.678574249109013e-08\n",
      "Loss 79.74134943391783\n",
      "Divergence 8.423556644625383e-08\n",
      "Loss 79.74150346638818\n",
      "Divergence 8.269242568801185e-08\n",
      "Loss 79.74162823430746\n",
      "Divergence 4.6067112412034004e-07\n",
      "Loss 79.74188071481768\n",
      "Divergence 7.688261301246535e-08\n",
      "Loss 79.7420397302261\n",
      "Divergence 4.2735700207341236e-07\n",
      "Loss 79.74218883982165\n",
      "Divergence 4.1585035242489715e-07\n",
      "Loss 79.74230413670793\n",
      "Divergence 7.204948358530182e-08\n",
      "Loss 79.74243287994193\n",
      "Divergence 3.973060741105719e-07\n",
      "Loss 79.74253812273938\n",
      "Divergence 3.8936346238188126e-07\n",
      "Loss 79.74266642232263\n",
      "Divergence 6.765817466366673e-08\n",
      "Loss 79.74281078822524\n",
      "Divergence 3.6991408819278884e-07\n",
      "Loss 79.7429212193411\n",
      "Divergence 6.512688475578815e-08\n",
      "Loss 79.74303018111628\n",
      "Divergence 6.375290394477215e-08\n",
      "Loss 79.74314833050545\n",
      "Divergence 3.465339192252649e-07\n",
      "Loss 79.74326050885996\n",
      "Divergence 6.140522165444745e-08\n",
      "Loss 79.74337730852697\n",
      "Divergence 6.038344696490412e-08\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "epsilon = 0.1\n",
    "gamma = 0.5\n",
    "alpha = 0.1\n",
    "\n",
    "Q = np.zeros((2,6))\n",
    "Q_next = np.zeros_like(Q)\n",
    "cc = np.zeros_like(Q)\n",
    "R = np.zeros_like(Q)\n",
    "R[0,1] = 1\n",
    "R[1,4] = 5\n",
    "n_states = Q.shape[1] - 1\n",
    "end_state = False\n",
    "\n",
    "sigma = 0.1\n",
    "\n",
    "# initial weights for the function approximation\n",
    "w = np.random.rand(13)\n",
    "\n",
    "# parameters of the Gaussian for the noise corruption of state transition. Mean and std\n",
    "mean_g = 0\n",
    "sigma_g = 0.1\n",
    "\n",
    "Loss = 0\n",
    "\n",
    "# basis functions in form (action x state x basis functions)\n",
    "phi_sa = np.zeros((2,6,13))\n",
    "phi_sa[:,:,12] = 1\n",
    "\n",
    "phi_s = np.abs(np.arange(6) - np.arange(6)[:,np.newaxis])\n",
    "phi_sa[0, :, 0:6] = phi_s\n",
    "phi_sa[1, :, 6:12] = phi_s\n",
    "\n",
    "for ii in range(epochs):\n",
    "    # set an arbitrary initial state. Continue if the initial state is also a terminal state.\n",
    "    s = np.rint(n_states * np.random.rand()).astype(int)\n",
    "    s_disc = s\n",
    "    if s==0 or s==5:\n",
    "            continue\n",
    "    while(not end_state):\n",
    "    #for i in range(1):        \n",
    "        # Apply ε-greedy research to determine the next action\n",
    "        t = np.random.rand()\n",
    "        if t<epsilon:\n",
    "            a = np.rint(np.random.rand()).astype(int)\n",
    "        else:\n",
    "            a = np.argmax([Q[:,s_disc]])\n",
    "            \n",
    "        # derive next continuous state, incorporating the Gaussian noise\n",
    "        # extract also a discretised next state. each state corresponds to the interval s+-0.5\n",
    "        s_next = 2*a - 1 + s + np.random.normal(mean_g, sigma_g)\n",
    "        s_next_disc = np.rint(s_next).astype(int)\n",
    "        if s_next_disc<0:\n",
    "            s_next_disc = 0\n",
    "        elif s_next_disc>Q.shape[1] - 1:\n",
    "            s_next_disc = Q.shape[1] - 1\n",
    "        \n",
    "        # Incrementing the counter of interactions with the certain (action, state) for a discretized version of the next state\n",
    "        # Next update the Q_target approximation, by division with the counter of (action, state)\n",
    "        cc[a, s_disc] += 1\n",
    "        Q_next[a, s_disc] = (R[a,s_disc] + gamma * max(Q[:, s_next_disc])) / cc[a, s_disc]\n",
    "        div = Q_next[a, s_disc] - np.multiply(w, phi_sa[a, s_disc, :])\n",
    "        Loss += 0.5 * div**2\n",
    "        w += alpha * np.multiply(div , phi_sa[a, s_disc, :])\n",
    "        \n",
    "        if s_next<0.5 or s_next>=4.5:\n",
    "            end_state = True\n",
    "            \n",
    "        s = s_next\n",
    "        s_disc = s_next_disc\n",
    "        Qtemp = np.array(Q, copy = True)\n",
    "        Q = np.array(Q_next, copy = True)\n",
    "    end_state = False\n",
    "    if ii%100 == 0 :\n",
    "        print('Loss', np.linalg.norm(Loss))\n",
    "        #print('Divergence', np.linalg.norm(Q-Qtemp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.61893906 4.61893906 3.61893906 2.61893906 1.61893906 0.61893906]\n",
      "[9.68921504e-123 5.02357268e-101 2.60457450e-079 1.35039518e-057\n",
      " 7.00140137e-036 3.63002044e-014]\n",
      "State 4.435542129782158\n",
      "Action 1\n",
      "Next state 5.61893906045828\n",
      "Loss 128144.53513347667\n"
     ]
    }
   ],
   "source": [
    "print(feat)\n",
    "print(phi)\n",
    "print('State', s)\n",
    "print(\"Action\", a)\n",
    "print('Next state', s_next)\n",
    "print('Loss', Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basis functions\n",
    "phi_sa = np.zeros((2,6,13))\n",
    "phi_sa[:,:,12] = 1\n",
    "\n",
    "phi_s = np.abs(np.arange(6) - np.arange(6)[:,np.newaxis])\n",
    "phi_sa[0, :, 0:6] = phi_s\n",
    "phi_sa[1, :, 6:12] = phi_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  4 12 24]\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine Learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
